---
title: "Estimating Equations in R: `geex`"
author: "B. Saul"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

M-estimation theory provides a framework for asympotic properties of estimators that are solutions to estimating equations. Regression methods such as Generalized Linear Models (GLM) and Generalized Estimating Equations (GEE) fit in this framework.  Countless R packages implement specific applications of estimating equations. A common reason to use M-estimation is to compute the empirical sandwich variance estimator - an asymptotically Normal and "robust" covariance. Many packages compute this variance estimator automatically, and packages such as `sandwich` take the output of other modeling methods to compute this variance estimate. 

`geex` aims to be provide a more general framework that any modelling method can use to compute point and variance estimates for parameters that are solutions to estimating equations. The basic idea:

* Analyst provides three things: (1) data, (2) instructions on how to split the data into independent units and (3) a function that takes unit-level data and returns a function in terms of parameters.
* `geex` computes point estimates and variance estimates for the parameters.

> Currently, `geex` does not compute point estimates. I am evaluating different optimizers for this purpose.

## Basic Setup

I mostly follow the notation of Stefanski and Boos. I tried to keep notation in the code similar to mathematical notation.

Suppose we have $m$ independent or nearly independent units of observations.

\[
\sum_{i = 1}^m \psi(O_i, \theta) = 0
\]

Where $\psi$ is vector of length $p$ corresponding to the number of parameters in $\theta$.

For notational ease, let $\psi(O_i, \theta) = \psi_i$ Let:
\[
A_i = - \frac{\partial \psi(O_i, \theta)}{\partial \theta}
\]

\[
A = \sum_{i = 1}^m A_i
\]

\[
B_i = \psi_i \psi_i^T
\]

\[
B = \sum_{i = 1}^m B_i
\]

\[
\Sigma = A^{-1} B \{A^{-1}\}^T
\]

## Small Sample Corrections of Fay (2001)

### Bias correction

\[
H_i = \{1 - min(b, \{A_i A\}_{jj}) \}^{-1/2}
\]
Where $b$ is a constant chosen by the analyst. Fay lets $b = 0.75$. Note that $H_i$ is a diagonal matrix.

\[
B^{bc}_i = H_i \psi_i \psi_i^T H_i
\]

\[
B^{bc} = \sum_{i = 1}^m B^{bc}_i
\]

\[
\Sigma^{bc} = A^{-1} B^{bc} \{A^{-1}\}^T
\]

### Degrees of Freedom corrections

Let $L$ be the contrast of interest (e.g.) $(0, \dots, 0, 1, -1)$ for a causal difference when the last two elements of the estimating equations are the counterfactual means.

\[
\mathcal{I} = [I_p \cdots I_p]
\]

where $I_p$ is a $p \times p$ identity matrix.

\[
G = I_{pm} - \begin{bmatrix}A^{bc}_1 \\ \vdots \\ A_m \end{bmatrix} A^{-1} \mathcal{I} 
\]

\[
M = diag\{H_i A^{-1} L L^T (A^{-1})^T H_i \}
\]

\[
C = G^T M G
\]

\[
w_i = L^T \left[ \left\{\sum_{j \neq i} A_i \right\}^{-1} - A^{-1} \right] L
\]

\[
\bar{w} = \sum_{i = 1}^m w_i
\]

\[
A^{bc}_i = \frac{w_i}{\bar{w}} B^{bc}
\]

\[
\hat{df}_1 = \frac{ \left\{ Tr( diag(A_i) C ) \right\}^2  }{ Tr( diag(A_i) C diag(A_i) C)}  
\]

\[
\hat{df}_2 = \frac{ \left\{ Tr( diag(A^{bc}_i) C ) \right\}^2  }{ Tr( diag(A^{bc}_i) C diag(A^{bc}_i) C)}  
\]

## Example: comparison to `sandwich`

I'll use the `vaccinesim` dataset.
```{r, echo = TRUE, message = FALSE, warning=FALSE}
library(eex)
library(dplyr)
library(inferference)
library(sandwich)
# library(microbenchmark)
```

An example $\psi$ function written in `R`. This function computes the score functions for a GLM.
```{r eefun, echo=TRUE}
eefun <- function(data, model){
  X <- model.matrix(model, data = data)
  Y <- model.response(model.frame(model, data = data))
  function(theta){
    lp  <- X %*% theta
    rho <- plogis(lp)

    score_eqns <- apply(X, 2, function(x) sum((Y - rho) * x))
    score_eqns
  }
}
```

Compare sandwich variance estimators to `sandwich` treating individuals as units:
```{r example1}
vaccinesim$ID <- 1:nrow(vaccinesim)
mglm    <- glm(A ~ X1, data = vaccinesim, family = binomial)
split_data  <- split(vaccinesim, vaccinesim$ID)
# The list needed for the compute_matrices
# For now, theta needs to be passed since geex does not do point estimates yet
example <- list(eeFUN = eefun, splitdt = split_data)
root <- eeroot(example, start = c(-.35, 0), model = mglm )$root

mats <- compute_matrices(obj = example, model = mglm,
                         theta = coef(mglm),
                         numDeriv_options = list(method = 'Richardson'))
# Compare point estimates
root # from GEEX
coef(mglm) # from the GLM function

# Compare variance estimates
compute_sigma(mats)
sandwich::sandwich(mglm)
```

Pretty darn good! Note that the `geex` method is much slower than `sandwich` (especially using `method = 'Richardson'` for `numDeriv`), but this is because `sandwich` uses the closed form of the score equations, while `geex` compute them numerically. However, `geex`'s real utility comes when you have more complicated estimating equations. Also, the analyst has the ability to code faster $\psi$ functions by optimizing their code or using `Rccp`, for example. 

## Example: IPW estimator of counterfactual mean

An example $\psi$ function written in `R`. This function computes the score functions for a GLM, plus two counterfactual means estimated by inverse probability weighting.

```{r eefun2, echo=TRUE}
eefun2 <- function(data, model, alpha){
  X <- model.matrix(model, data = data)
  A <- model.response(model.frame(model, data = data))
  
  function(theta){
    p  <- length(theta)
    p1 <- length(coef(model))
    lp  <- X %*% theta[1:p1]
    rho <- plogis(lp)

    hh  <- (rho/alpha)^A * ((1-rho)/(1-alpha))^(1 - A)
    IPW <- 1/(exp(sum(log(hh))))

    score_eqns <- apply(X, 2, function(x) sum((A - rho) * x))
    with(data, {
      ce0 <- mean(Y * (A == 0)) * IPW
      ce1 <- mean(Y * (A == 1)) * IPW
      
      c(score_eqns,
        ce0 - theta[p - 1],
        ce1 - theta[p])
    })
  }
}
```

Compare to what `inferference` gets.

```{r example2, echo =TRUE}
test <- interference(Y | A ~ X1 | group, 
                     data = vaccinesim,
                     model_method = 'glm',
                     allocations = c(.35, .4))

mglm        <- glm(A ~ X1, data = vaccinesim, family = binomial)
split_data  <- split(vaccinesim, vaccinesim$group)

# The list needed for the compute_matrices
example <- list(eeFUN   = eefun2, 
                splitdt = split_data)

root <- eeroot(example, start =c(coef(mglm), .4,  .13), model = mglm, alpha = .35 )$root
                # Plugging in the values of CE_hat(0) and CE_hat(1)
                # since geex does not currently do point estimates)

mats <- compute_matrices(obj = example, 
                         numDeriv_options = list(method = 'Richardson'),
                         theta   = c(coef(mglm), 0.42186669,  0.15507946),
                # Plugging in the values of CE_hat(0) and CE_hat(1)
                # since geex returns incorrect point estimates for this problem at the moment
                         model = mglm, 
                         alpha = .35)
# conpare SE estimates
L <- c(0, 0, -1, 1)
Sigma <- compute_sigma(mats)
sqrt(t(L) %*% Sigma %*% L)  # from GEEX
direct_effect(test, allocation = .35)$std.error # from inferference
```


Fairly close. I would expect them to be somewhat different, since `inferference` computes the variance with the block diagonal trick in the Perez appendix.

```{r,  echo = TRUE}
### Point Estimate via inferference
c(coef(mglm), 0.42186669,  0.15507946)

### Points Estimates with GEEX
root
```
Not so good. What happens if we start the geex root solver at the inferference estimates?

```{r, echo = TRUE}
eeroot(example, start =c(coef(mglm), 0.42186669,  0.15507946), model = mglm, alpha = .35 )$root
```
Hmmm, still not so good for the causal effects


## Stefanski \& Boos example 1

```{r SB_example1, echo=TRUE}
n  <- 100
dt <- data.frame(Y = rnorm(n, mean = 5, sd = 4), id = 1:n)
split_data <- split(dt, dt$id)

SB_ex1_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2] )
    )
  }
}

example <- list(eeFUN   = SB_ex1_eefun, 
                splitdt = split_data)

root <- eeroot(example, start = c(1,1))$root

mats  <- compute_matrices(obj = example,
                          theta = root,
                          numDeriv_options = list(method = 'Richardson'))
Sigma <- compute_sigma(mats)

## Compare to closed form ##

A <- diag(1, nrow = 2)

B <- with(dt, {
  Ybar <- mean(Y)
  B11 <- mean( (Y - Ybar)^2 )
  B12 <- mean( (Y - Ybar) * ((Y - Ybar)^2 - B11) )
  B22 <- mean( ((Y - Ybar)^2 - B11)^2 )
  matrix(
    c(B11, B12,
      B12, B22), nrow = 2
  )
})

## geex roots 
root

## closed form roots
# note that var() divides by n - 1, not n
summarize(dt, p1 = mean(Y), p2 = var(Y))

# geex variance estimate
Sigma

# closed form
(solve(A) %*% B %*% t(solve(A))) / n

```


## Stefanski \& Boos example 2

```{r SB_example2, echo=TRUE}
n  <- 100
dt <- data.frame(Y  = rnorm(n, mean = 5, sd = 4), 
                 X  = rnorm(n, mean = 2, sd = .09),
                 id = 1:n)
split_data <- split(dt, dt$id)

SB_ex2_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
        X - theta[2],
        theta[1] - (theta[3] * theta[2]) )
    )
  }
}

example <- list(eeFUN   = SB_ex2_eefun, 
                splitdt = split_data)
root <- eeroot(obj = example, start = c(1, 1, 1))$root
mats  <- compute_matrices(obj = example,
                          theta = root,
                          numDeriv_options = list(method = 'Richardson'))
Sigma <- compute_sigma(mats)

## Compare to closed form ##

A <- with(dt, {
 matrix(
  c(1 , 0, 0,
    0 , 1, 0,
    -1, mean(Y)/mean(X), mean(X)),
  byrow = TRUE, nrow = 3)
})

B <- with(dt, {
  matrix(
    c(var(Y)   , cov(Y, X), 0,
      cov(Y, X), var(X)   , 0,
      0, 0, 0),
    byrow = TRUE, nrow = 3)
})

## geex roots 
root

## closed form roots
summarize(dt, p1 = mean(Y), p2 = mean(X), p3 = p1/p2)

# geex variance estimate
Sigma

# closed form
(solve(A) %*% B %*% t(solve(A))) / n

```

## Stefanski \& Boos example 3

Closed form variance is complicated, so didn't do for now.

```{r SB_example3, echo=TRUE}
set.seed(100) # running into issue where sqrt(theta2) and log(theta2) return NaN for some seeds
n  <- 100
dt <- data.frame(Y  = rnorm(n, mean = 5, sd = 4), 
                 id = 1:n)
split_data <- split(dt, dt$id)

SB_ex3_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2],
       sqrt(theta[2]) - theta[3],
       log(theta[2]) - theta[4])
    )
  }
}

example <- list(eeFUN   = SB_ex3_eefun, 
                splitdt = split_data)
root <- eeroot(obj = example, start = c(1, 1, 1, 1))$root


mats  <- compute_matrices(obj = example,
                          theta = root,
                          numDeriv_options = list(method = 'Richardson'))
Sigma <- compute_sigma(mats)

## geex roots 
root

## closed form roots
summarize(dt, p1 = mean(Y), p2 = sum((Y - p1)^2)/n(), p3 = sqrt(p2), p4 = log(p2))

# geex variance estimate
Sigma


```


## Stefanski \& Boos example 4

```{r SB_example4, echo=TRUE}
n  <- 10000

# Oracle parms
alpha <- 2
beta  <- 3
gamma <- 2
delta <- 1.5
e1 <- e2 <- e3 <- rnorm(n)
sigma_e <- 1
sigma_U <- .25
sigma_tau <- 1
### Random variables

X <- rgamma(n, shape = 5)
X <- rnorm(n, sd = 1)

dt <- data.frame(Y  = alpha + (beta * X) + (sigma_e * e1), 
                 W  = X + (sigma_U * e2),
                 T_  = gamma + (delta * X) + (sigma_tau * e3),
                 id = 1:n)
split_data <- split(dt, dt$id)

SB_ex4_eefun <- function(data){
  function(theta){
    with(data,
      c(theta[1] - T_,
        theta[2] - W,
        (Y - (theta[3] * W)) * (theta[2] - W),
        (Y - (theta[4] * W)) * (theta[1] - T_))
    )
  }
}



example <- list(eeFUN   = SB_ex4_eefun, 
                splitdt = split_data)
root <- eeroot(obj = example, start = c(1, 1, 1, 1))$root
root

## compare to closed form
c(theta1 = mean(dt$T_),
  theta2 = mean(dt$W),
  theta3 = coef(lm(Y ~ W, data = dt))[2],
  theta4 = coef(lm(Y ~ T_, data = dt))[2]/coef(lm(W ~ T_, data = dt))[2])


mats  <- compute_matrices(obj = example,
                          theta = root,
                          numDeriv_options = list(method = 'Richardson'))
Sigma <- compute_sigma(mats)

Sigma


## compare to closed form
# TODO

```

## Stefanski \& Boos example 5

```{r SB_example5, echo=TRUE}

### NOT WORKING ####


n <- 100
theta0 <- 0
dt <- data.frame(X = rnorm(n, mean = 2),
                 id = 1:n)
split_data <- split(dt, dt$id)

distr <- function(x, theta0){
  FF <- ecdf(x - theta0)
  approxfun(x - theta0, y = FF(x - theta0), method = "linear",
            0, 1, rule = 1, f = 0, ties = mean)
}

FF <- distr(dt$X, 0)

dens <- density(dt$X)

ff2 <- approxfun(dens$x, dens$y, yleft = 0, yright = 0)
plot(xx, ff2(xx))

integrand <- function(y){
  ff2(y)^2
}

IC_denom <- integrate(integrand, lower = min(dt$X), upper = max(dt$X))$value

SB_ex5_eefun <- function(data){
  Xi <- data$X
  function(theta){
    print(FF(Xi - theta[1]))
     IC_HL <- (FF(Xi - theta[1]) - 0.5)/IC_denom
     c(IC_HL,
       Xi - theta[1])
  }
}

split_data <- split(dt, dt$id)

example <- list(eeFUN   = SB_ex5_eefun, 
                splitdt = split_data)

root <- eeroot(obj = example, start = c(1, 1))$root

median(dt$X)
mean(dt$X)
```

