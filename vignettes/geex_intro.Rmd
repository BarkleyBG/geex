---
title: "Estimating Equations in R: `geex`"
author: "B. Saul"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
- \usepackage{float}
vignette: >
  %\VignetteIndexEntry{geex intro}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(dplyr)
library(inferference)
library(sandwich)
library(xtable)
library(moments)
library(MASS)
library(knitr)
# library(rprojroot)
# child.path <- normalizePath(paste0(find_package_root_file(), '/vignettes/examples/'))
opts_knit$set(progress = TRUE, verbose = TRUE, child.path = 'examples/')
# library(microbenchmark)
```

M-estimation theory provides a framework for asympotic properties of estimators that are solutions to estimating equations. Regression methods such as Generalized Linear Models (GLM) and Generalized Estimating Equations (GEE) fit in this framework.  Countless R packages implement specific applications of estimating equations. A common reason to use M-estimation is to compute the empirical sandwich variance estimator - an asymptotically Normal and "robust" covariance. Many packages compute this variance estimator automatically, and packages such as `sandwich` take the output of other modeling methods to compute this variance estimate. 

`geex` aims to be provide a more general framework that any modelling method can use to compute point and variance estimates for parameters that are solutions to estimating equations. The basic idea:

* Analyst provides three things: (1) data, (2) instructions on how to split the data into independent units and (3) a function that takes unit-level data and returns a function in terms of parameters.
* `geex` computes point estimates and variance estimates for the parameters.

## Basic Setup

I mostly follow the notation of Stefanski and Boos. I tried to keep notation in the code similar to mathematical notation.

Suppose we have $m$ independent or nearly independent units of observations.

\[
\sum_{i = 1}^m \psi(O_i, \theta) = 0
\]

Where $\psi$ is vector of length $p$ corresponding to the number of parameters in $\theta$.

For notational ease, let $\psi(O_i, \theta) = \psi_i$ Let:
\[
A_i = - \frac{\partial \psi(O_i, \theta)}{\partial \theta}
\]

\[
A = \sum_{i = 1}^m A_i
\]

\[
B_i = \psi_i \psi_i^T
\]

\[
B = \sum_{i = 1}^m B_i
\]

\[
\Sigma = A^{-1} B \{A^{-1}\}^T
\]


```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(dplyr)
library(inferference)
library(sandwich)
library(xtable)
# library(microbenchmark)
```

```{r functions_results, echo = FALSE}
print_pmatrix <- function(object, digits = 4){
  if(!is.matrix(object)){
    object <- matrix(object, nrow = 1)
  }
  
  paste0('$', print(xtable::xtable(object, align=rep("",ncol(object)+1), digits =digits), comment = FALSE,
        floating=FALSE, tabular.environment="pmatrix", hline.after=NULL, 
        include.rownames=FALSE, include.colnames=FALSE, print.results = FALSE), '$')
}

first_diff_dec <- function(x){
  -floor(log10(abs(x)))
}

print_results <- function(results, label, caption){
  r <- results
  cat('\\begin{table}[H] \n',
      '\\centering \n',
      '\\label{', label, '} \n',
      '\\caption{"', caption, '"} \n',
      '\\begin{tabular}{lcc} \n',
      ' & $\\hat{\\theta}$ & $\\hat{\\Sigma}$  \\\\ \n',
      'Closed form &', print_pmatrix(r$cls$parameters),  '&', print_pmatrix(r$cls$vcov), '\\\\ \n',
      'geex &',  print_pmatrix(r$geex$parameters),  '&', print_pmatrix(r$geex$vcov), '\\\\ \n',
      'Decimal of difference &',  print_pmatrix(first_diff_dec(r$cls$parameters - r$geex$parameters), d = 0),  '&',
                                  print_pmatrix(first_diff_dec(r$cls$vcov - r$geex$vcov), d = 0), '\\\\ \n',
      '\\end{tabular} \n', 
      '\\end{table}')
}
```


## Stefanski \& Boos example 1


```{r SB1_setup, echo=FALSE}
n  <- 100
mu <- 5
sigma <- 2
dt <- data.frame(Y = rnorm(n, mean = mu, sd = sigma), id = 1:n)
```

Example 1 illustrates calculation of sample mean and variance using estimating equations. I generate a data set with `r n` observations drawn from a Normal(`r mu`, `r sigma`) distribution. Table \ref{ex1} translates the estimating equations into the `R` function needed for `geex`:


\begin{table}[H]
\centering
\label{ex1}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi(Y_i, \theta) = 
\begin{pmatrix}
Y_i - \theta_1 \\
(Y_i - \theta_1)^2 - \theta_2
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB1_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2] )
    )
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB1_eefun, echo=FALSE, results='hide'}
SB1_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2] )
    )
  }
}
```

With the `eeFUN` function prepared, it is passed to `estimate_equations` along with the data, a character string naming the variable that identifies groups within the dataset, and starting values for the root finder.

```{r SB1_run, echo=TRUE}
estimates <- estimate_equations(eeFUN = SB1_eefun, 
                                data  = dt,
                                units = 'id', 
                                roots = c(1,1))
```

```{r SB1_clsform, echo=FALSE}
## Compare to closed form ##

A <- diag(1, nrow = 2)

B <- with(dt, {
  Ybar <- mean(Y)
  B11 <- mean( (Y - Ybar)^2 )
  B12 <- mean( (Y - Ybar) * ((Y - Ybar)^2 - B11) )
  B22 <- mean( ((Y - Ybar)^2 - B11)^2 )
  matrix(
    c(B11, B12,
      B12, B22), nrow = 2
  )
})

## closed form roots
# note that var() divides by n - 1, not n
theta_cls <- summarize(dt, p1 = mean(Y), p2 = var(Y) * (n() - 1)/ n() )

# closed form
Sigma_cls <- (solve(A) %*% B %*% t(solve(A))) / n
```

```{r SB1_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))

print_results(results, 'ex1', 'Comparing estimates from closed form versus geex')
```



## Stefanski \& Boos example 2



```{r SB2_setup, echo=FALSE}
n  <- 100
muY <- 5
sigmaY <- 2
muX <- 2
sigmaX <- 0.2
dt <- data.frame(Y  = rnorm(n, mean = muY, sd = sigmaY), 
                 X  = rnorm(n, mean = muX, sd = sigmaX),
                 id = 1:n)
```

Example 2 illustrates calculation of a ratio estimator. I generate a data set with `r n` observations where $Y \sim N$(`r muY`, `r sigmaY`) and $X \sim N$(`r muX`, `r sigmaX`). Table \ref{ex2} translates the estimating equations into the `R` function needed for `geex`:


\begin{table}[H]
\centering
\label{ex2}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi(Y_i, \theta) = 
\begin{pmatrix}
Y_i - \theta_1 \\
X_i - \theta_2 \\
\theta_1 - \theta_3\theta_2
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB2_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
        X - theta[2],
        theta[1] - (theta[3] * theta[2]) )
    )
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB2_eefun, echo = FALSE}
SB2_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
        X - theta[2],
        theta[1] - (theta[3] * theta[2]) )
    )
  }
}
```

```{r SB2_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB2_eefun, 
                                data  = dt, units = 'id', 
                                roots = c(1, 1, 1))
```

```{r SB2_clsform, echo = FALSE}
## Compare to closed form ##

A <- with(dt, {
 matrix(
  c(1 , 0, 0,
    0 , 1, 0,
    -1, mean(Y)/mean(X), mean(X)),
  byrow = TRUE, nrow = 3)
})

B <- with(dt, {
  matrix(
    c(var(Y)   , cov(Y, X), 0,
      cov(Y, X), var(X)   , 0,
      0, 0, 0),
    byrow = TRUE, nrow = 3)
})

## closed form roots
theta_cls <- summarize(dt, p1 = mean(Y), p2 = mean(X), p3 = p1/p2)

## closed form covariance
Sigma_cls <- (solve(A) %*% B %*% t(solve(A))) / n
```

```{r SB2_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'test', 'test')
```



## Stefanski \& Boos example 3

```{r SB3_setup, echo=FALSE}
n  <- 100
mu <- 5
sigma <- 4
set.seed(100) # running into issue where sqrt(theta2) and log(theta2) return NaN for some seeds
dt <- data.frame(Y  = rnorm(n, mean = mu, sd = sigma), 
                 id = 1:n)
```

Example 3 illustrates calculation of a ratio estimator. I generate a data set with `r n` observations where $Y \sim N$(`r mu`, `r sigma`). Table \ref{ex3} translates the estimating equations into the `R` function needed for `geex`:


\begin{table}[H]
\centering
\label{ex3}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi(Y_i, \theta) = 
\begin{pmatrix}
Y_i - \theta_1 \\
X_i - \theta_2 \\
\sqrt{\theta_2} - \theta_3 \\
log(\theta_2) - \theta_4
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB3_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2],
       sqrt(theta[2]) - theta[3],
       log(theta[2]) - theta[4])
    )
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB3_eefun, echo = FALSE}
SB3_eefun <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2],
       sqrt(theta[2]) - theta[3],
       log(theta[2]) - theta[4])
    )
  }
}
```

```{r SB3_run, echo = TRUE}
estimates <- estimate_equations(eeFUN= SB3_eefun, 
                                data  = dt, units = 'id', 
                                roots = c(1, 1, 1, 1))
```

```{r SB3_clsform, echo = FALSE}
## closed form roots
theta_cls <- summarize(dt, p1 = mean(Y), p2 = sum((Y - p1)^2)/n(), p3 = sqrt(p2), p4 = log(p2))

## Compare to closed form ##
theta2 <- theta_cls$p2
mu3 <- moments::moment(dt$Y, order = 3, central = TRUE)
mu4 <- moments::moment(dt$Y, order = 4, central = TRUE)
# A <- matrix(c(1, 0, 0, 0,
#               0, 1, 0, 0,
#               0, -1/(2 * sqrt(theta2)), 1, 0,
#               0, -1/theta2, 0, 1), 
#             byrow = TRUE, nrow = 4)
# B <- matrix(c(1/theta2, mu3/(2 * theta2^3), 0, 0,
#               mu3/(2 * theta2^3), (mu4 - theta2^2)/(4 * theta2^4), 0, 0,
#               0, 0, 0, 0,
#               0, 0, 0, 0),
#             byrow = TRUE, nrow = 4)

## closed form covariance
Sigma_cls <- matrix(
  c(theta2, mu3, mu3/(2*sqrt(theta2)), mu3/theta2,
    mu3, mu4 - theta2^2, (mu4 - theta2^2)/(2*sqrt(theta2)), (mu4 - theta2^2)/theta2,
    mu3/(2 * sqrt(theta2)), (mu4 - theta2^2)/(2*sqrt(theta2)), (mu4 - theta2^2)/(4*theta2), (mu4 - theta2^2)/(2*theta2^(3/2)),
    mu3/theta2, (mu4 - theta2^2)/theta2, (mu4 - theta2^2)/(2*theta2^(3/2)), (mu4/theta2^2) - 1) ,
  nrow = 4, byrow = TRUE) / n
## closed form covariance
# Sigma_cls <- (solve(A) %*% B %*% t(solve(A))) / n
```


```{r SB3_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'ex3_results', 'Example 3')
```


## Stefanski \& Boos example 4



```{r SB4_setup, echo=FALSE}
n  <- 100

# Oracle parms
alpha <- 2
beta  <- 3
gamma <- 2
delta <- 1.5
e1 <- e2 <- e3 <- rnorm(n)
sigma_e <- 1
sigma_U <- .25
sigma_tau <- 1
### Random variables

X <- rgamma(n, shape = 5)
# X <- rnorm(n, sd = 1)

dt <- data.frame(Y  = alpha + (beta * X) + (sigma_e * e1), 
                 W  = X + (sigma_U * e2),
                 T_  = gamma + (delta * X) + (sigma_tau * e3),
                 id = 1:n)
```

Example 4 illustrates calculation of an instumental variable estimator. I generate a data set with `r n` observations where INPUT data generation. Table \ref{ex4} translates the estimating equations into the `R` function needed for `geex`:


\begin{table}[H]
\centering
\label{ex4}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi(Y_i, T_i, W_i \theta) = 
\begin{pmatrix}
\theta_1 - T_i\\
\theta_2 - W_i \\
(Y_i - \theta_3 W_i)(\theta_2 - W_i) \\
(Y_i - \theta_4 W_i)(\theta_1 - T_i) \\
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB_eefun <- function(data){
  function(theta){
    with(data,
      c(theta[1] - T_,
        theta[2] - W,
        (Y - (theta[3] * W)) * (theta[2] - W),
        (Y - (theta[4] * W)) * (theta[1] - T_))
    )
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB4_eefun, echo = FALSE}
SB4_eefun <- function(data){
  function(theta){
    with(data,
      c(theta[1] - T_,
        theta[2] - W,
        (Y - (theta[3] * W)) * (theta[2] - W),
        (Y - (theta[4] * W)) * (theta[1] - T_))
    )
  }
}
```

```{r SB4_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB4_eefun, 
                                data  = dt, units = 'id', 
                                roots = c(1, 1, 1, 1))
```

```{r SB4_clsform, echo = TRUE}
YW_model <- lm(Y ~ W, data = dt)
YT_model <- lm(Y ~ T_, data = dt)

WT_model <- lm(W ~ T_, data = dt)
## closed form roots
theta_cls <- c(theta1 = mean(dt$T_),
  theta2 = mean(dt$W),
  theta3 = coef(YW_model)[2],
  theta4 = coef(YT_model)[2]/coef(WT_model)[2])

## closed form covariance
# Not sure how compute SB's closed form since it depends on X, which is
# supposed to be unobserved.
# sigma2_e <- var(residuals(YW_model))
# sigma2_W <- var(dt$W)
# sigma2_U <- var(residuals(YT_model))

# ((sigma_e^2 * (1 + sigma_U^2 * sigma_e^2) + beta^2 * (sigma_U^2 * 1))/(1 + sigma_U^2 * sigma_e^2)^2) / (100^2)
# ((sigma_e^2 * (1 + sigma_U^2 * sigma_e^2) + beta^2 * (sigma_U^2 * 1))/(1 + sigma_U^2 * sigma_e^2)^2) / (100^2)
Sigma_cls <- matrix(NA, nrow = 2, ncol = 2)
```

```{r SB4_results, echo = FALSE, results = 'asis'}
# primary interest lies in the lower 2 x 2 submatrix of the asymptotic variance matrix
estimates$vcov <- estimates$vcov[3:4, 3:4]
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))

print_results(results, 'Example 4', 'Example 4')
```


## Stefanski \& Boos example 5



```{r SB5_setup, echo=FALSE}
library(ICSNP)
n <- 100
theta0 <- 0
theta_tru <- 2
sigma <- 1
dt <- data.frame(X = rnorm(n, mean = 2, sd = sigma),
                 id = 1:n)
```

Example 5 illustrates calculation of an instumental variable estimator. I generate a data set with `r n` observations where $X \sim N$(`r theta_tru`, `r sigma`). Let $\theta_0 =$ `r theta0`. Table \ref{ex5} translates the estimating equations for the Hodges-Lehmann location estimation and the sample mean into the `R` function needed for `geex`:

\begin{table}[H]
\centering
\label{ex5}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi(Y_i, \theta) = 
\begin{pmatrix}
IC_{\hat{\theta}_{HL}}(X; \theta_0) - (\theta_1 - \theta_0) \\
X_i - \theta_2 \\
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB5_eefun <- function(data, theta0 = 0){
  Xi <- data$X
  IC_HL <- (1/IC_denom) * (F0(Xi, theta0) - 0.5)
  function(theta){
     c(IC_HL - (theta[1] - theta0), 
       Xi - theta[2]) 
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB5_eefun, echo = TRUE}
F0 <- function(y, theta0, distrFUN = pnorm){
  distrFUN(y - theta0, mean = 0)
}

f0 <- function(y, densFUN){
  densFUN(y, mean = 0)
}

integrand <- function(y, densFUN = dnorm){
  f0(y, densFUN = densFUN)^2
}

IC_denom <- integrate(integrand, lower = -Inf, upper = Inf)$value

SB5_eefun <- function(data){
  Xi <- data$X
  function(theta){
     IC_HL <- (1/IC_denom) * (F0(Xi, theta[1]) - 0.5)
     c(IC_HL,
       Xi - theta[2]) 
  }
}
```

```{r SB5_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB5_eefun, 
                                data  = dt, units = 'id', 
                                roots = c(2, 1))
```

```{r SB5_clsform, echo = TRUE}
theta_cls <- c(hl.loc(dt$X), mean(dt$X))

## closed form covariance
# Not sure how compute SB's closed form since it depends on X, which is
# supposed to be unobserved.
Sigma_cls <- matrix(c(1/(12 * IC_denom^2) / n, NA, NA, var(dt$X)/100), 
                    nrow = 2, ncol = 2, byrow = TRUE)
```

```{r SB5_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'Example 5', 'Example 5')
```


## Stefanski \& Boos example 6


```{r SB6_setup, echo=FALSE}
n <- 100
theta_tru <- 2
sigma <- 1
dt <- data.frame(Y = rnorm(n, mean = 2, sd = sigma),
                 id = 1:n)
```

Example 6 illustrates calculation of the Huber estimator of the center of symmetric distributions. I generate a data set with `r n` observations where $Y \sim N$(`r theta_tru`, `r sigma`). Table \ref{ex6} translates the estimating equations for the Huber estimator for the center of symmetric distributions into the `R` function needed for `geex`:

\begin{table}[H]
\centering
\label{ex6}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi_k(Y_i, \theta) = 
\begin{pmatrix}
(Y_i - \theta) * I(|(Y_i - \theta)| \leq k) + k * sgn(Y_i - \theta)\\
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB6_eefun <- function(data, k = 1.5){
  function(theta){
    x <- data$Y - theta[1]
    if(abs(x) <= k) x else sign(x) * k
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB6_eefun, echo = FALSE}
SB6_eefun <- function(data, k = 1.5){
  function(theta){
    x <- data$Y - theta[1]
    if(abs(x) <= k) x else sign(x) * k
  }
}
```

```{r SB6_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB6_eefun, 
                                data  = dt, units = 'id', 
                                roots = 1)
```

```{r SB6_clsform, echo = TRUE}
theta_cls <- MASS::huber(dt$Y, tol = 1e-10)$mu

psi_k <- function(x, k = 1.5){
  if(abs(x) <= k) x else sign(x) * k
}

A <- lapply(dt$Y, function(y){
  x <- y - theta_cls
  -numDeriv::grad(psi_k, x = x)
}) %>% unlist() %>% mean()

B <- lapply(dt$Y, function(y){
  x <- y - theta_cls
  psi_k(x = x)^2
}) %>% unlist() %>% mean()

## closed form covariance
Sigma_cls <- matrix(1/A * B * 1/A / n)
```

```{r SB6_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'Example 6', 'Example 6')
```



## Stefanski \& Boos example 8


```{r SB8_setup, echo=FALSE}
n <- 50
beta <- c(0.5, 2)
dt <- data_frame(X  = rep(0:1, each = n/2),
                 e  = rnorm(n),
                 Y  = as.numeric(cbind(1, X) %*% beta) + e,
                 id = 1:n)
```

Example 8 illustrates robust regression. I generate a data set with `r n` observations where half of the observation have $X_i = 1$ and the others have $X_i = 0$. $Y = $ `r beta[1]` $+$ `r beta[2]` $X_i + \epsilon_i$ and $\epsilon_i \sim N(0, 1)$. Table \ref{ex8} translates the estimating equations for robust regression into the `R` function needed for `geex`:

\begin{table}[H]
\centering
\label{ex8}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi_k(Y_i, \theta) = 
\begin{pmatrix}
\psi_k(Y_i - \mathbf{x}_i^T \beta) \mathbf{x}_i
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB8_eefun <- function(data){
  Yi <- data$Y
  xi <- model.matrix(Y ~ X, data = data)
  function(theta){
    r <- Yi - xi %*% theta
    c(psi_k(r) %*% xi)
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB8_eefun, echo = FALSE}
psi_k <- function(x, k = 1.345){
  if(abs(x) <= k) x else sign(x) * k
}

SB8_eefun <- function(data){
  Yi <- data$Y
  xi <- model.matrix(Y ~ X, data = data)
  function(theta){
    r <- Yi - xi %*% theta
    c(psi_k(r) %*% xi)
  }
}
```

```{r SB8_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB8_eefun, 
                                data  = dt, units = 'id', 
                                roots = c(1, 1))
```

```{r SB8_clsform, echo = TRUE}
m <- MASS::rlm(Y ~ X, data = dt, method = 'M')
theta_cls <- coef(m)
Sigma_cls <- vcov(m)

```

```{r SB8_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'Example 8', 'Example 8')
```

## Stefanski \& Boos example 9


```{r SB9_setup, echo=FALSE}
n <- 100
beta <- c(0.5, 2, .1)
dt <- data_frame(X1 = rep(0:1, each = n/2), 
                 X2 = rep(0:1, times = n/2),
                 Y  = rbinom(n, 1, prob = as.numeric(plogis(cbind(1, X1, X2) %*% beta))),
                 id = 1:n)
```

Example 9 illustrates estimation of a generalized linear model. I generate a data set with `r n` observations where half of the observation have $X_{1i} = 1$ and the others have $X_{1i} = 0$. $Y_i \sim Bern[\mbox{logit}^{-1}($ `r beta[1]` $+$ `r beta[2]` $X_{1i} +$ `r beta[3]` $X_{2i})]$. Table \ref{ex9} translates the estimating equations for logistic regression into the `R` function needed for `geex`:

\begin{table}[H]
\centering
\label{ex9}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi_k(Y_i, \theta) = 
\begin{pmatrix}
D_i(\beta)\frac{Y_i - \mu_i(\beta)}{V_i(\beta) \tau}
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB9_eefun <- function(data){
  Yi <- data$Y
  xi <- model.matrix(Y ~ X1 + X2, data = data, drop = FALSE)
  function(theta){
    lp <- xi %*% theta
    mu <- plogis(lp)
    D  <- t(xi) %*% dlogis(lp)
    V  <- mu * (1 - mu)
    D %*% solve(V) %*% (Yi - mu)
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB9_eefun, echo = FALSE}
SB9_eefun <- function(data){
  Yi <- data$Y
  xi <- model.matrix(Y ~ X1 + X2, data = data, drop = FALSE)
  function(theta){
    lp <- xi %*% theta
    mu <- plogis(lp)
    D  <- t(xi) %*% dlogis(lp)
    V  <- mu * (1 - mu)
    D %*% solve(V) %*% (Yi - mu)
  }
}
```

```{r SB9_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB9_eefun, 
                                data  = dt, units = 'id', 
                                roots = c(1, 1, 1))
```

```{r SB9_clsform, echo = TRUE}
m <- glm(Y ~ X1 + X2, data = dt, family = binomial(link = 'logit'))
theta_cls <- coef(m)
Sigma_cls <- sandwich(m)

```

```{r SB9_results, echo = FALSE, results = 'asis'}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'Example 9', 'Example 9')
```

## Stefanski \& Boos example 10


```{r SB10_setup, echo=FALSE}
shaq <- data_frame(game = 1:23,
                 ft_made = c(4, 5, 5, 5, 2, 7, 6, 9, 4, 1, 13, 5, 6, 9, 7, 3, 8, 1, 18, 3, 10, 1, 3),
                 ft_attp = c(5, 11, 14, 12, 7, 10, 14, 15, 12, 4, 27, 17, 12, 9, 12, 10, 12, 6, 39, 13, 17, 6, 12))
```

Example 10 illustrates testing equality of success probablities. Table \ref{ex10} translates the estimating equations into the `R` function needed for `geex`:

\begin{table}[H]
\centering
\label{ex10}
\caption{Translating math to code}
\begin{tabular}{cc}
$\psi_k(Y_i, n_i, \theta) = 
\begin{pmatrix}
\frac{(Y_i - n_i \theta_2)^2}{n_i \theta_2( 1 - \theta_2 )} - \theta_1 \\
Y_i - n_i \theta_2
\end{pmatrix}$ &
\begin{minipage}{3in}
\begin{verbatim}
SB10_eefun <- function(data){
  Y <- data$ft_made
  n <- data$ft_attp
  function(theta){
    p <- theta[2]
    c(((Y - (n * p))^2)/(n * p * (1 - p))  - theta[1], 
      Y - n * p)
  }
}
\end{verbatim}
\end{minipage}
\end{tabular}
\end{table}

```{r SB10_eefun, echo = FALSE}
SB10_eefun <- function(data){
  Y <- data$ft_made
  n <- data$ft_attp
  function(theta){
    p <- theta[2]
    c(((Y - (n * p))^2)/(n * p * (1 - p))  - theta[1], 
      Y - n * p)
  }
}
```

```{r SB10_run, echo = TRUE}
estimates <- estimate_equations(eeFUN = SB10_eefun, 
                                data  = shaq, 
                                units = 'game', 
                                numDeriv_options = list(method.args = list(eps = 1e-7, r = 10, zero.tol = .Machine$double.eps)), 
                                roots = c(.5, .5))
```

```{r SB10_clsform, echo = TRUE}
V11 <- function(p) {
  k    <- nrow(shaq)
  sumn <- sum(shaq$ft_attp)
  sumn_inv <- sum(1/shaq$ft_attp)
  term2_n  <- 1 - (6 * p) + (6 * p^2)
  term2_d <- p * (1 - p) 
  term2  <- term2_n/term2_d
  term3  <- ((1 - (2 * p))^2) / ((sumn/k) * p * (1 - p))
  2 + (term2 * (1/k) * sumn_inv) - term3
}

p_tilde <- sum(shaq$ft_made)/sum(shaq$ft_attp)
V11_hat <- V11(p_tilde)/23

# Compare variance estimates
V11_hat
estimates$vcov[1, 1]

# Compare p-values
pnorm(35.51/23, mean  = 1, sd = sqrt(V11_hat), lower.tail = FALSE)

pnorm(estimates$parameters[1], 
      mean = 1, 
      sd = sqrt(estimates$vcov[1, 1]),
      lower.tail = FALSE)

```

```{r SB10_results, echo = FALSE, results = 'asis', eval = FALSE}
results <- list(geex = estimates, cls = list(parameters = theta_cls, vcov = Sigma_cls))
print_results(results, 'Example 10', 'Example 10')
```


# Small Sample Corrections of Fay (2001)

## Bias correction

\[
H_i = \{1 - min(b, \{A_i A\}_{jj}) \}^{-1/2}
\]
Where $b$ is a constant chosen by the analyst. Fay lets $b = 0.75$. Note that $H_i$ is a diagonal matrix.

\[
B^{bc}_i = H_i \psi_i \psi_i^T H_i
\]

\[
B^{bc} = \sum_{i = 1}^m B^{bc}_i
\]

\[
\Sigma^{bc} = A^{-1} B^{bc} \{A^{-1}\}^T
\]

### Degrees of Freedom corrections

Let $L$ be the contrast of interest (e.g.) $(0, \dots, 0, 1, -1)$ for a causal difference when the last two elements of the estimating equations are the counterfactual means.

\[
\mathcal{I} = [I_p \cdots I_p]
\]

where $I_p$ is a $p \times p$ identity matrix.

\[
G = I_{pm} - \begin{bmatrix}A^{bc}_1 \\ \vdots \\ A_m \end{bmatrix} A^{-1} \mathcal{I} 
\]

\[
M = diag\{H_i A^{-1} L L^T (A^{-1})^T H_i \}
\]

\[
C = G^T M G
\]

\[
w_i = L^T \left[ \left\{\sum_{j \neq i} A_i \right\}^{-1} - A^{-1} \right] L
\]

\[
\bar{w} = \sum_{i = 1}^m w_i
\]

\[
A^{bc}_i = \frac{w_i}{\bar{w}} B^{bc}
\]

\[
\hat{df}_1 = \frac{ \left\{ Tr( diag(A_i) C ) \right\}^2  }{ Tr( diag(A_i) C diag(A_i) C)}  
\]

\[
\hat{df}_2 = \frac{ \left\{ Tr( diag(A^{bc}_i) C ) \right\}^2  }{ Tr( diag(A^{bc}_i) C diag(A^{bc}_i) C)}  
\]

