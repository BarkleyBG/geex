---
title: "Estimating Equations in R: `geex`"
author: "B. Saul"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
- \usepackage{float}
vignette: >
  %\VignetteIndexEntry{geex intro}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(dplyr)
library(inferference)
library(sandwich)
library(xtable)
library(moments)
library(MASS)
library(knitr)
# library(rprojroot)
# child.path <- normalizePath(paste0(find_package_root_file(), '/vignettes/examples/'))
opts_knit$set(progress = TRUE, verbose = TRUE, child.path = 'examples/')
# library(microbenchmark)
```

M-estimation theory provides a framework for asympotic properties of estimators that are solutions to estimating equations. Regression methods such as Generalized Linear Models (GLM) and Generalized Estimating Equations (GEE) fit in this framework.  Countless R packages implement specific applications of estimating equations. A common reason to use M-estimation is to compute the empirical sandwich variance estimator - an asymptotically Normal and "robust" covariance. Many packages compute this variance estimator automatically, and packages such as `sandwich` take the output of other modeling methods to compute this variance estimate. 

`geex` aims to be provide a more general framework that any modelling method can use to compute point and variance estimates for parameters that are solutions to estimating equations. The basic idea:

* Analyst provides three things: (1) data, (2) instructions on how to split the data into independent units and (3) a function that takes unit-level data and returns a function in terms of parameters.
* `geex` computes point estimates and variance estimates for the parameters.

## Basic Setup

I mostly follow the notation of Stefanski and Boos. I tried to keep notation in the code similar to mathematical notation.

Suppose we have $m$ independent or nearly independent units of observations.

\[
\sum_{i = 1}^m \psi(O_i, \theta) = 0
\]

Where $\psi$ is vector of length $p$ corresponding to the number of parameters in $\theta$.

For notational ease, let $\psi(O_i, \theta) = \psi_i$ Let:
\[
A_i = - \frac{\partial \psi(O_i, \theta)}{\partial \theta}
\]

\[
A = \sum_{i = 1}^m A_i
\]

\[
B_i = \psi_i \psi_i^T
\]

\[
B = \sum_{i = 1}^m B_i
\]

\[
\Sigma = A^{-1} B \{A^{-1}\}^T
\]


```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(dplyr)
library(inferference)
library(sandwich)
library(xtable)
# library(microbenchmark)
```

```{r functions_results, echo = FALSE}
print_pmatrix <- function(object, digits = 4){
  if(!is.matrix(object)){
    object <- matrix(object, nrow = 1)
  }
  
  paste0('$', print(xtable(object, align=rep("",ncol(object)+1), digits =digits), comment = FALSE,
        floating=FALSE, tabular.environment="pmatrix", hline.after=NULL, 
        include.rownames=FALSE, include.colnames=FALSE, print.results = FALSE), '$')
}

first_diff_dec <- function(x){
  -floor(log10(abs(x)))
}

print_results <- function(results, label, caption){
  r <- results
  cat('\\begin{table}[H] \n',
      '\\centering \n',
      '\\label{', label, '} \n',
      '\\caption{"', caption, '"} \n',
      '\\begin{tabular}{lcc} \n',
      ' & $\\hat{\\theta}$ & $\\hat{\\Sigma}$  \\\\ \n',
      'Closed form &', print_pmatrix(r$cls$parameters),  '&', print_pmatrix(r$cls$vcov), '\\\\ \n',
      'geex &',  print_pmatrix(r$geex$parameters),  '&', print_pmatrix(r$geex$vcov), '\\\\ \n',
      'Decimal of difference &',  print_pmatrix(first_diff_dec(r$cls$parameters - r$geex$parameters), d = 0),  '&',
                                  print_pmatrix(first_diff_dec(r$cls$vcov - r$geex$vcov), d = 0), '\\\\ \n',
      '\\end{tabular} \n', 
      '\\end{table}')
}
```


## Stefanski \& Boos example 1

```{r child = 'SB1.Rmd'}
```


## Stefanski \& Boos example 2

```{r child = 'SB2.Rmd'}
```


## Stefanski \& Boos example 3
```{r child = 'SB3.Rmd'}
```

## Stefanski \& Boos example 4

```{r child = 'SB4.Rmd'}
```

## Stefanski \& Boos example 5

```{r child = 'SB5.Rmd'}
```

## Stefanski \& Boos example 6

```{r child = 'SB6.Rmd'}
```

## Stefanski \& Boos example 7

```{r child = 'SB7.Rmd'}
```

## Stefanski \& Boos example 8

```{r child = 'SB8.Rmd'}
```

## Stefanski \& Boos example 9

```{r child = 'SB9.Rmd'}
```

## Stefanski \& Boos example 10

```{r child = 'SB10.Rmd'}
```


# Small Sample Corrections of Fay (2001)

## Bias correction

\[
H_i = \{1 - min(b, \{A_i A\}_{jj}) \}^{-1/2}
\]
Where $b$ is a constant chosen by the analyst. Fay lets $b = 0.75$. Note that $H_i$ is a diagonal matrix.

\[
B^{bc}_i = H_i \psi_i \psi_i^T H_i
\]

\[
B^{bc} = \sum_{i = 1}^m B^{bc}_i
\]

\[
\Sigma^{bc} = A^{-1} B^{bc} \{A^{-1}\}^T
\]

### Degrees of Freedom corrections

Let $L$ be the contrast of interest (e.g.) $(0, \dots, 0, 1, -1)$ for a causal difference when the last two elements of the estimating equations are the counterfactual means.

\[
\mathcal{I} = [I_p \cdots I_p]
\]

where $I_p$ is a $p \times p$ identity matrix.

\[
G = I_{pm} - \begin{bmatrix}A^{bc}_1 \\ \vdots \\ A_m \end{bmatrix} A^{-1} \mathcal{I} 
\]

\[
M = diag\{H_i A^{-1} L L^T (A^{-1})^T H_i \}
\]

\[
C = G^T M G
\]

\[
w_i = L^T \left[ \left\{\sum_{j \neq i} A_i \right\}^{-1} - A^{-1} \right] L
\]

\[
\bar{w} = \sum_{i = 1}^m w_i
\]

\[
A^{bc}_i = \frac{w_i}{\bar{w}} B^{bc}
\]

\[
\hat{df}_1 = \frac{ \left\{ Tr( diag(A_i) C ) \right\}^2  }{ Tr( diag(A_i) C diag(A_i) C)}  
\]

\[
\hat{df}_2 = \frac{ \left\{ Tr( diag(A^{bc}_i) C ) \right\}^2  }{ Tr( diag(A^{bc}_i) C diag(A^{bc}_i) C)}  
\]

