---
title: "Estimating Equations in R: `geex`"
author: "B. Saul"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(dplyr)
library(inferference)
library(sandwich)
library(xtable)
library(moments)
# library(microbenchmark)
```

M-estimation theory provides a framework for asympotic properties of estimators that are solutions to estimating equations. Regression methods such as Generalized Linear Models (GLM) and Generalized Estimating Equations (GEE) fit in this framework.  Countless R packages implement specific applications of estimating equations. A common reason to use M-estimation is to compute the empirical sandwich variance estimator - an asymptotically Normal and "robust" covariance. Many packages compute this variance estimator automatically, and packages such as `sandwich` take the output of other modeling methods to compute this variance estimate. 

`geex` aims to be provide a more general framework that any modelling method can use to compute point and variance estimates for parameters that are solutions to estimating equations. The basic idea:

* Analyst provides three things: (1) data, (2) instructions on how to split the data into independent units and (3) a function that takes unit-level data and returns a function in terms of parameters.
* `geex` computes point estimates and variance estimates for the parameters.

## Basic Setup

I mostly follow the notation of Stefanski and Boos. I tried to keep notation in the code similar to mathematical notation.

Suppose we have $m$ independent or nearly independent units of observations.

\[
\sum_{i = 1}^m \psi(O_i, \theta) = 0
\]

Where $\psi$ is vector of length $p$ corresponding to the number of parameters in $\theta$.

For notational ease, let $\psi(O_i, \theta) = \psi_i$ Let:
\[
A_i = - \frac{\partial \psi(O_i, \theta)}{\partial \theta}
\]

\[
A = \sum_{i = 1}^m A_i
\]

\[
B_i = \psi_i \psi_i^T
\]

\[
B = \sum_{i = 1}^m B_i
\]

\[
\Sigma = A^{-1} B \{A^{-1}\}^T
\]


```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(dplyr)
library(inferference)
library(sandwich)
library(xtable)
# library(microbenchmark)
```

```{r functions_results, echo = FALSE}
print_pmatrix <- function(object, digits = 4){
  if(!is.matrix(object)){
    object <- matrix(object, nrow = 1)
  }
  
  paste0('$', print(xtable(object, align=rep("",ncol(object)+1), digits =digits), comment = FALSE,
        floating=FALSE, tabular.environment="pmatrix", hline.after=NULL, 
        include.rownames=FALSE, include.colnames=FALSE, print.results = FALSE), '$')
}

first_diff_dec <- function(x){
  -floor(log10(abs(x)))
}

print_results <- function(results, label, caption){
  r <- results
  cat('\\begin{table}[!htbp] \n',
      '\\centering \n',
      '\\label{', label, '} \n',
      '\\caption{"', caption, '"} \n',
      '\\begin{tabular}{lcc} \n',
      ' & $\\hat{\\theta}$ & $\\hat{\\Sigma}$  \\\\ \n',
      'Closed form &', print_pmatrix(r$cls$parameters),  '&', print_pmatrix(r$cls$vcov), '\\\\ \n',
      'geex &',  print_pmatrix(r$geex$parameters),  '&', print_pmatrix(r$geex$vcov), '\\\\ \n',
      'Decimal of difference &',  print_pmatrix(first_diff_dec(r$cls$parameters - r$geex$parameters), d = 0),  '&',
                                  print_pmatrix(first_diff_dec(r$cls$vcov - r$geex$vcov), d = 0), '\\\\ \n',
      '\\end{tabular} \n', 
      '\\end{table}')
}
```


## Stefanski \& Boos example 1

```{r child = 'examples/SB1.Rmd'}
```


## Stefanski \& Boos example 2

```{r child = 'examples/SB2.Rmd'}
```


## Stefanski \& Boos example 3
```{r child = 'examples/SB3.Rmd'}
```

## Stefanski \& Boos example 4

```{r child = 'examples/SB4.Rmd'}
```

## Stefanski \& Boos example 5

```{r child = 'examples/SB5.Rmd'}
```


## Stefanski \& Boos example 6

```{r child = 'examples/SB6.Rmd'}
```




## Stefanski \& Boos example 7

```{r SB_example7, echo=TRUE}


```

## Stefanski \& Boos example 8

```{r SB_example8, echo=TRUE}
n <- 50
beta <- c(0.5, 2)
dt <- data_frame(X  = rep(0:1, each = n/2),
                 e  = rnorm(n),
                 Y  = as.numeric(cbind(1, X) %*% beta) + e,
                 id = 1:n)
split_data <- split(dt, dt$id)

psi_k <- function(x, k = 1.345){
  if(abs(x) <= k) x else sign(x) * k
}

SB_ex8_eefun <- function(data){
  Yi <- data$Y
  xi <- model.matrix(Y ~ X, data = data)
  function(theta){
    r <- Yi - xi %*% theta
    c(psi_k(r) %*% xi)
  }
}

example <- list(eeFUN   = SB_ex8_eefun, 
                splitdt = split_data)

root <- eeroot(obj = example, start = c(1, 2))$root


m <- MASS::rlm(Y ~ X, data = dt, method = 'M')

# Compare paramter estimates
root # GEEX
coef(m) # rlm

# Asymp correlation btn HL estimator and mean
mats  <- compute_matrices(obj = example,
                          theta = root,
                          numDeriv_options = list(method = 'Richardson'))
Sigma <- compute_sigma(mats)

# Compare variance
Sigma
vcov(m)

```


## Stefanski \& Boos example 9

```{r SB_example9, echo=TRUE}
n <- 100
beta <- c(0.5, 2, .1)

dt <- data_frame(X1 = rep(0:1, each = n/2), 
                 X2 = rep(0:1, times = n/2),
                 Y  = rbinom(n, 1, prob = as.numeric(plogis(cbind(1, X1, X2) %*% beta))),
                 id = 1:n)
split_data <- split(dt, dt$id)

SB_ex9_eefun <- function(data){
  Yi <- data$Y
  xi <- model.matrix(Y ~ X1 + X2, data = data, drop = FALSE)
  function(theta){
    lp <- xi %*% theta
    mu <- plogis(lp)
    D  <- t(xi) %*% dlogis(lp)
    V  <- mu * (1 - mu)
    D %*% solve(V) %*% (Yi - mu)
  }
}

# SB_ex9_eefun(split_data[[6]])(c(.5, 2, .1))

example <- list(eeFUN   = SB_ex9_eefun, 
                splitdt = split_data)

root <- eeroot(obj = example, start = c(.5, 2, 1))$root
m <- glm(Y ~ X1 + X2, data = dt, family = binomial(link = 'logit'))

# Compare parameter estimates
root
coef(m)
mats <- compute_matrices(example, theta = root)
Sigma <- compute_sigma(mats)

# Compare variance estimates
Sigma
sandwich(m)
```



## Stefanski \& Boos example 10

```{r SB_example10, echo=TRUE}

dt <- data_frame(game = 1:23,
                 ft_made = c(4, 5, 5, 5, 2, 7, 6, 9, 4, 1, 13, 5, 6, 9, 7, 3, 8, 1, 18, 3, 10, 1, 3),
                 ft_attp = c(5, 11, 14, 12, 7, 10, 14, 15, 12, 4, 27, 17, 12, 9, 12, 10, 12, 6, 39, 13, 17, 6, 12))

split_data <- split(dt, dt$game)

SB_ex10_eefun <- function(data){
  Y <- data$ft_made
  n <- data$ft_attp
  function(theta){
    p <- theta[2]
    c(((Y - (n * p))^2)/(n * p * (1 - p))  - theta[1], 
      Y - n * p)
  }
}
SB_ex10_eefun(split_data[[1]])(c(.5, .5))
example <- list(eeFUN   = SB_ex10_eefun, 
                splitdt = split_data)

root <- eeroot(obj = example, start = c(.5, .5))$root
root

V11 <- function(p) {
  k <- length(nrow(dt))
  sumn <- sum(dt$ft_attp)
  sumn_inv <- sum(1/dt$ft_attp)
  term2_n <- 1 - (6 * p) + (6 * p^2)
  term2_d <- p * (1 - p) 
  term2 <- term2_n/term2_d
  print(term2)
  term3 <- ((1 - 2 * p)^2)/( (sumn/k) * p * (1 - p))
  print(term3)
  2 + (term2 * (1/k) * sumn_inv)  - term3
}

### ???? I keep getting a negative value for V11

p_tilde <- sum(dt$ft_made)/sum(dt$ft_attp)
V <- V11(.45)
V
pnorm(root[1], mean = 1, sd = sqrt(V))

```


## Small Sample Corrections of Fay (2001)

### Bias correction

\[
H_i = \{1 - min(b, \{A_i A\}_{jj}) \}^{-1/2}
\]
Where $b$ is a constant chosen by the analyst. Fay lets $b = 0.75$. Note that $H_i$ is a diagonal matrix.

\[
B^{bc}_i = H_i \psi_i \psi_i^T H_i
\]

\[
B^{bc} = \sum_{i = 1}^m B^{bc}_i
\]

\[
\Sigma^{bc} = A^{-1} B^{bc} \{A^{-1}\}^T
\]

### Degrees of Freedom corrections

Let $L$ be the contrast of interest (e.g.) $(0, \dots, 0, 1, -1)$ for a causal difference when the last two elements of the estimating equations are the counterfactual means.

\[
\mathcal{I} = [I_p \cdots I_p]
\]

where $I_p$ is a $p \times p$ identity matrix.

\[
G = I_{pm} - \begin{bmatrix}A^{bc}_1 \\ \vdots \\ A_m \end{bmatrix} A^{-1} \mathcal{I} 
\]

\[
M = diag\{H_i A^{-1} L L^T (A^{-1})^T H_i \}
\]

\[
C = G^T M G
\]

\[
w_i = L^T \left[ \left\{\sum_{j \neq i} A_i \right\}^{-1} - A^{-1} \right] L
\]

\[
\bar{w} = \sum_{i = 1}^m w_i
\]

\[
A^{bc}_i = \frac{w_i}{\bar{w}} B^{bc}
\]

\[
\hat{df}_1 = \frac{ \left\{ Tr( diag(A_i) C ) \right\}^2  }{ Tr( diag(A_i) C diag(A_i) C)}  
\]

\[
\hat{df}_2 = \frac{ \left\{ Tr( diag(A^{bc}_i) C ) \right\}^2  }{ Tr( diag(A^{bc}_i) C diag(A^{bc}_i) C)}  
\]

