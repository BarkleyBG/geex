---
title: "An introduction to M-estimation with `geex`"
author: "B. Saul"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to geex}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(geex)
library(knitr)
opts_knit$set(progress = TRUE, verbose = TRUE)
```

M-estimation theory provides a framework for asympotic properties of estimators that are solutions to estimating equations. Regression methods such as Generalized Linear Models (GLM) and Generalized Estimating Equations (GEE) fit in this framework.  Countless R packages implement specific applications of estimating equations. A common reason to use M-estimation is to compute the empirical sandwich variance estimator - an asymptotically Normal and "robust" covariance. Many packages compute this variance estimator automatically, and packages such as `sandwich` take the output of other modeling methods to compute this variance estimate. 

`geex` aims to be provide a more general framework that any modelling method can use to compute point and variance estimates for parameters that are solutions to estimating equations. The basic idea:

* Analyst provides three things: (1) data, (2) instructions on how to split the data into independent units and (3) a function that takes unit-level data and returns a function in terms of parameters.
* `geex` computes point estimates and variance estimates for the parameters.

## Basic Setup

I mostly follow the notation of Stefanski and Boos. I tried to keep notation in the code similar to mathematical notation.

Suppose we have $m$ independent or nearly independent units of observations.

\[
\sum_{i = 1}^m \psi(O_i, \theta) = 0
\]

Where $\psi$ is vector of length $p$ corresponding to the number of parameters in $\theta$.

For notational ease, let $\psi(O_i, \theta) = \psi_i$ Let:
\[
A_i = - \frac{\partial \psi(O_i, \theta)}{\partial \theta}
\]

\[
A = \sum_{i = 1}^m A_i
\]

\[
B_i = \psi_i \psi_i^T
\]

\[
B = \sum_{i = 1}^m B_i
\]

\[
\Sigma = A^{-1} B \{A^{-1}\}^T
\]


## Stefanski \& Boos example 1


```{r SB1_setup, echo=FALSE}
n  <- 100
mu <- 5
sigma <- 2
dt <- data.frame(Y = rnorm(n, mean = mu, sd = sigma), id = 1:n)
```

Example 1 illustrates calculation of sample mean and variance using estimating equations. I generate a data set with `r n` observations drawn from a Normal(`r mu`, `r sigma`) distribution. 

We translate the estimating equations for the mean and variance:

\[
\psi(Y_i, \theta) = 
\begin{pmatrix}
Y_i - \theta_1 \\
(Y_i - \theta_1)^2 - \theta_2
\end{pmatrix}
\]

into an `estFUN`:

```{r SB1_estFUN, echo=TRUE, results='hide'}
SB1_estFUN <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2] )
    )
  }
}
```

With the `estFUN` function prepared, it is passed to `m_estimate` along with the data, a character string naming the variable that identifies groups within the dataset, and starting values for the root finder.

```{r SB1_run, echo=TRUE}
estimates <- m_estimate(
  estFUN = SB1_estFUN, 
  data  = dt,
  root_control = setup_root_solver(start = c(1,1)))
```

```{r SB1_clsform, echo=FALSE}
## Compare to closed form ##

A <- diag(1, nrow = 2)

B <- with(dt, {
  Ybar <- mean(Y)
  B11 <- mean( (Y - Ybar)^2 )
  B12 <- mean( (Y - Ybar) * ((Y - Ybar)^2 - B11) )
  B22 <- mean( ((Y - Ybar)^2 - B11)^2 )
  matrix(
    c(B11, B12,
      B12, B22), nrow = 2
  )
})

## closed form roots
# note that var() divides by n - 1, not n
theta_cls <- dplyr::summarize(dt, p1 = mean(Y), p2 = var(Y) * (n() - 1)/ n() )

# closed form
Sigma_cls <- (solve(A) %*% B %*% t(solve(A))) / n
```

```{r SB1_results, echo = FALSE}
results <- list(geex = list(parameters = coef(estimates), vcov = vcov(estimates)), 
                cls = list(parameters = theta_cls, vcov = Sigma_cls))
results
```



## Stefanski \& Boos example 2

```{r SB2_setup, echo=FALSE}
n  <- 100
muY <- 5
sigmaY <- 2
muX <- 2
sigmaX <- 0.2
dt <- data.frame(Y  = rnorm(n, mean = muY, sd = sigmaY), 
                 X  = rnorm(n, mean = muX, sd = sigmaX),
                 id = 1:n)
```

Example 2 illustrates calculation of a ratio estimator. I generate a data set with `r n` observations where $Y \sim N$(`r muY`, `r sigmaY`) and $X \sim N$(`r muX`, `r sigmaX`).

Estimating equations:
\[
\psi(Y_i, \theta) = 
\begin{pmatrix}
Y_i - \theta_1 \\
X_i - \theta_2 \\
\theta_1 - \theta_3\theta_2
\end{pmatrix}
\]

Corresponding `estFUN`:
```{r SB2_estFUN, echo = TRUE}
SB2_estFUN <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
        X - theta[2],
        theta[1] - (theta[3] * theta[2]) )
    )
  }
}
```

```{r SB2_run, echo = TRUE}
estimates <- m_estimate(
  estFUN = SB2_estFUN, 
  data  = dt, 
  root_control = setup_root_solver(start = c(1, 1, 1)))
```

```{r SB2_clsform, echo = FALSE}
## Compare to closed form ##

A <- with(dt, {
 matrix(
  c(1 , 0, 0,
    0 , 1, 0,
    -1, mean(Y)/mean(X), mean(X)),
  byrow = TRUE, nrow = 3)
})

B <- with(dt, {
  matrix(
    c(var(Y)   , cov(Y, X), 0,
      cov(Y, X), var(X)   , 0,
      0, 0, 0),
    byrow = TRUE, nrow = 3)
})

## closed form roots
theta_cls <- dplyr::summarize(dt, p1 = mean(Y), p2 = mean(X), p3 = p1/p2)

## closed form covariance
Sigma_cls <- (solve(A) %*% B %*% t(solve(A))) / n
```

```{r SB2_results, echo = FALSE}
results <- list(geex = list(parameters = coef(estimates), vcov = vcov(estimates)), 
                cls = list(parameters = theta_cls, vcov = Sigma_cls))
results
```

## Stefanski \& Boos example 3

```{r SB3_setup, echo=FALSE}
n  <- 100
mu <- 5
sigma <- 4
set.seed(100) # running into issue where sqrt(theta2) and log(theta2) return NaN for some seeds
dt <- data.frame(Y  = rnorm(n, mean = mu, sd = sigma), 
                 id = 1:n)
```

Example 3 illustrates calculation of a ratio estimator. I generate a data set with `r n` observations where $Y \sim N$(`r mu`, `r sigma`). 

Estimating equations:
\[
\psi(Y_i, \theta) = 
\begin{pmatrix}
Y_i - \theta_1 \\
X_i - \theta_2 \\
\sqrt{\theta_2} - \theta_3 \\
log(\theta_2) - \theta_4
\end{pmatrix}
\]

Corresponding `estFUN`:
```{r SB3_estFUN, echo = TRUE}
SB3_estFUN <- function(data){
  function(theta){
    with(data,
      c(Y - theta[1],
       (Y - theta[1])^2 - theta[2],
       sqrt(theta[2]) - theta[3],
       log(theta[2]) - theta[4])
    )
  }
}
```

```{r SB3_run, echo = TRUE}
estimates <- m_estimate(
  estFUN= SB3_estFUN, 
  data  = dt,
  root_control = setup_root_solver(start = c(1, 1, 1, 1)))
```

```{r SB3_clsform, echo = FALSE}
## closed form roots
theta_cls <- dplyr::summarize(dt, p1 = mean(Y), p2 = sum((Y - p1)^2)/n(), p3 = sqrt(p2), p4 = log(p2))

## Compare to closed form ##
theta2 <- theta_cls$p2
mu3 <- moments::moment(dt$Y, order = 3, central = TRUE)
mu4 <- moments::moment(dt$Y, order = 4, central = TRUE)
# A <- matrix(c(1, 0, 0, 0,
#               0, 1, 0, 0,
#               0, -1/(2 * sqrt(theta2)), 1, 0,
#               0, -1/theta2, 0, 1), 
#             byrow = TRUE, nrow = 4)
# B <- matrix(c(1/theta2, mu3/(2 * theta2^3), 0, 0,
#               mu3/(2 * theta2^3), (mu4 - theta2^2)/(4 * theta2^4), 0, 0,
#               0, 0, 0, 0,
#               0, 0, 0, 0),
#             byrow = TRUE, nrow = 4)

## closed form covariance
Sigma_cls <- matrix(
  c(theta2, mu3, mu3/(2*sqrt(theta2)), mu3/theta2,
    mu3, mu4 - theta2^2, (mu4 - theta2^2)/(2*sqrt(theta2)), (mu4 - theta2^2)/theta2,
    mu3/(2 * sqrt(theta2)), (mu4 - theta2^2)/(2*sqrt(theta2)), (mu4 - theta2^2)/(4*theta2), (mu4 - theta2^2)/(2*theta2^(3/2)),
    mu3/theta2, (mu4 - theta2^2)/theta2, (mu4 - theta2^2)/(2*theta2^(3/2)), (mu4/theta2^2) - 1) ,
  nrow = 4, byrow = TRUE) / n
## closed form covariance
# Sigma_cls <- (solve(A) %*% B %*% t(solve(A))) / n
```


```{r SB3_results, echo = FALSE}
results <- list(geex = list(parameters = coef(estimates), vcov = vcov(estimates)), 
                cls = list(parameters = theta_cls, vcov = Sigma_cls))
results
```

